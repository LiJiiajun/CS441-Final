{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyP4k/Y4iOJpyX4DxJFCeozQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CS 441 final project"],"metadata":{"id":"Bix_Ml0nfaRb"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import time\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, models\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.decomposition import PCA\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","DRIVE_BASE = \"/content/drive/My Drive/CS441/Final/garbage_dataset\"\n","LOCAL_TRAIN = \"/content/garbage_train\"\n","LOCAL_TEST  = \"/content/garbage_test\"\n","\n","FORCE_SYNC = False\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","!pip -q install pillow-heif\n","from PIL import Image\n","import pillow_heif\n","pillow_heif.register_heif_opener()\n","\n","import os, subprocess\n","\n","IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".heic\", \".heif\")\n","\n","def count_images(root):\n","    if not os.path.exists(root):\n","        return 0\n","    c = 0\n","    for _, _, files in os.walk(root):\n","        for f in files:\n","            if f.lower().endswith(IMG_EXTS):\n","                c += 1\n","    return c\n","\n","def rsync_dir(src, dst):\n","    cmd = [\"rsync\", \"-a\", \"--delete\", \"--info=progress2\", src.rstrip(\"/\") + \"/\", dst.rstrip(\"/\") + \"/\"]\n","    print(\"Running:\", \" \".join([f'\"{x}\"' if \" \" in x else x for x in cmd]))\n","    subprocess.run(cmd, check=True)\n","\n","drive_train = os.path.join(DRIVE_BASE, \"train\")\n","drive_test  = os.path.join(DRIVE_BASE, \"test\")\n","\n","if not os.path.exists(drive_train):\n","    raise FileNotFoundError(f\"Not found: {drive_train}\")\n","if not os.path.exists(drive_test):\n","    raise FileNotFoundError(f\"Not found: {drive_test}\")\n","\n","drive_train_cnt = count_images(drive_train)\n","drive_test_cnt  = count_images(drive_test)\n","\n","local_train_cnt = count_images(LOCAL_TRAIN)\n","local_test_cnt  = count_images(LOCAL_TEST)\n","\n","print(f\"Drive train images: {drive_train_cnt} | Local train images: {local_train_cnt}\")\n","print(f\"Drive test  images: {drive_test_cnt}  | Local test  images: {local_test_cnt}\")\n","\n","need_sync = FORCE_SYNC or (drive_train_cnt != local_train_cnt) or (drive_test_cnt != local_test_cnt)\n","print(\"Need sync:\", need_sync)\n","\n","if need_sync:\n","    rsync_dir(drive_train, LOCAL_TRAIN)\n","    rsync_dir(drive_test,  LOCAL_TEST)\n","\n","train_root = LOCAL_TRAIN\n","test_root  = LOCAL_TEST\n","\n","local_train_cnt2 = count_images(train_root)\n","local_test_cnt2  = count_images(test_root)\n","\n","print(\"\\n=== READY ===\")\n","print(\"train_root =\", train_root, \"| images:\", local_train_cnt2)\n","print(\"test_root  =\", test_root,  \"| images:\", local_test_cnt2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xYUT2Z5A3un","executionInfo":{"status":"ok","timestamp":1765767701649,"user_tz":360,"elapsed":3362,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"f798a455-8651-4b9c-8b6c-c870365265f8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive train images: 20371 | Local train images: 20371\n","Drive test  images: 251  | Local test  images: 251\n","Need sync: False\n","\n","=== READY ===\n","train_root = /content/garbage_train | images: 20371\n","test_root  = /content/garbage_test | images: 251\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKlmXnejjgST","executionInfo":{"status":"ok","timestamp":1765767701710,"user_tz":360,"elapsed":16,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"209e9739-8574-4f55-a74a-40a1b8f56ac9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"O8ddF2qP2jnD","executionInfo":{"status":"ok","timestamp":1765767701716,"user_tz":360,"elapsed":1,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"outputs":[],"source":["class GarbageFineDataset(Dataset):\n","    def __init__(self, root_dir, transform=None,\n","                 fine_class_to_idx=None, big_class_to_idx=None,\n","                 strict=False,\n","                 extensions=('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.heic', '.heif')):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.extensions = extensions\n","        self.strict = strict\n","\n","        self.fine_class_to_idx = {} if fine_class_to_idx is None else dict(fine_class_to_idx)\n","        self.big_class_to_idx  = {} if big_class_to_idx  is None else dict(big_class_to_idx)\n","\n","        self.samples = []  # (image_path, fine_idx, big_idx)\n","\n","        big_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n","        for big_name in big_names:\n","            big_path = os.path.join(root_dir, big_name)\n","\n","            if big_name in self.big_class_to_idx:\n","                big_idx = self.big_class_to_idx[big_name]\n","            else:\n","                if self.strict:\n","                    raise ValueError(f\"[STRICT] test/show train/not exist big category: {big_name}\")\n","                big_idx = len(self.big_class_to_idx)\n","                self.big_class_to_idx[big_name] = big_idx\n","\n","            fine_names = sorted([d for d in os.listdir(big_path) if os.path.isdir(os.path.join(big_path, d))])\n","            for fine_name in fine_names:\n","                fine_path = os.path.join(big_path, fine_name)\n","\n","                fine_full_name = f\"{big_name}/{fine_name}\"\n","\n","                if fine_full_name in self.fine_class_to_idx:\n","                    fine_idx = self.fine_class_to_idx[fine_full_name]\n","                else:\n","                    if self.strict:\n","                        raise ValueError(f\"[[STRICT] test/show train/not exist small category: {fine_full_name}\")\n","                    fine_idx = len(self.fine_class_to_idx)\n","                    self.fine_class_to_idx[fine_full_name] = fine_idx\n","\n","\n","                for fname in os.listdir(fine_path):\n","                    if fname.lower().endswith(self.extensions):\n","                        img_path = os.path.join(fine_path, fname)\n","                        self.samples.append((img_path, fine_idx, big_idx))\n","\n","        print(f\"[{root_dir}] samples={len(self.samples)}, big={len(self.big_class_to_idx)}, fine={len(self.fine_class_to_idx)}\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, fine_idx, big_idx = self.samples[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, fine_idx, big_idx\n","\n","    @property\n","    def fine_idx_to_name(self):\n","        return {v: k for k, v in self.fine_class_to_idx.items()}\n","\n","    @property\n","    def big_idx_to_name(self):\n","        return {v: k for k, v in self.big_class_to_idx.items()}\n","\n","    @property\n","    def fine_to_big(self):\n","        # fine_idx -> big_idx\n","        mapping = {}\n","        for _, f_idx, b_idx in self.samples:\n","            mapping[f_idx] = b_idx\n","        return mapping"]},{"cell_type":"code","source":["img_size = 224\n","batch_size = 32\n","num_workers = 4\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomResizedCrop(img_size),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_test_transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","# 0.9 / 0.1\n","full_train_dataset = GarbageFineDataset(train_root, transform=train_transform)\n","\n","full_train_for_val = GarbageFineDataset(\n","    train_root,\n","    transform=val_test_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","seed = 42\n","g = torch.Generator().manual_seed(seed)\n","\n","N = len(full_train_dataset)\n","n_train = int(0.9 * N)\n","n_val = N - n_train\n","train_subset, val_subset = random_split(range(N), [n_train, n_val], generator=g)\n","\n","\n","class IndexSubset(Dataset):\n","    def __init__(self, base_dataset, indices):\n","        self.base = base_dataset\n","        self.indices = list(indices)\n","    def __len__(self):\n","        return len(self.indices)\n","    def __getitem__(self, i):\n","        return self.base[self.indices[i]]\n","\n","train_dataset = IndexSubset(full_train_dataset, train_subset)\n","val_dataset   = IndexSubset(full_train_for_val, val_subset)\n","\n","test_dataset = GarbageFineDataset(\n","    test_root,\n","    transform=val_test_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n","val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","fine_idx_to_name = full_train_dataset.fine_idx_to_name\n","big_idx_to_name  = full_train_dataset.big_idx_to_name\n","fine_to_big      = full_train_dataset.fine_to_big\n","\n","print(\"Big classes:\", big_idx_to_name)\n","print(\"Fine classes (examples):\", list(fine_idx_to_name.items())[:11])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWkxRwuK_J4l","executionInfo":{"status":"ok","timestamp":1765767701843,"user_tz":360,"elapsed":116,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"be7b1f1f-9f59-4c69-bec9-3174135b588f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_test] samples=251, big=4, fine=16\n","Big classes: {0: 'recycling', 1: 'special', 2: 'trash', 3: 'yard_waste'}\n","Fine classes (examples): [(0, 'recycling/cardboard'), (1, 'recycling/glass'), (2, 'recycling/metal'), (3, 'recycling/paper'), (4, 'recycling/plastic'), (5, 'special/battery'), (6, 'special/cables'), (7, 'special/keyboard'), (8, 'special/mouse'), (9, 'special/tire'), (10, 'trash/biological')]\n"]}]},{"cell_type":"markdown","source":["# 1. SVM"],"metadata":{"id":"aMI6h4VYtADL"}},{"cell_type":"code","source":["svm_transform = transforms.Compose([\n","    transforms.Resize((96, 96)),\n","    transforms.ToTensor(),\n","])\n","svm_full_train = GarbageFineDataset(\n","    train_root,\n","    transform=svm_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","svm_train_dataset = IndexSubset(svm_full_train, train_subset)\n","svm_val_dataset   = IndexSubset(svm_full_train, val_subset)\n","\n","svm_test_dataset = GarbageFineDataset(\n","    test_root,\n","    transform=svm_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","svm_batch_size = 64\n","svm_train_loader = DataLoader(svm_train_dataset, batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","svm_val_loader   = DataLoader(svm_val_dataset,   batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","svm_test_loader  = DataLoader(svm_test_dataset,  batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","\n","\n","\n","@torch.no_grad()\n","def extract_flat_features(loader):\n","    feats, labels = [], []\n","    for images, fine_labels, _ in loader:\n","        flat = images.view(images.size(0), -1)\n","        feats.append(flat.cpu().numpy())\n","        labels.append(fine_labels.numpy())\n","    return np.concatenate(feats), np.concatenate(labels)\n","\n","\n","svm_train_feats, svm_train_labels = extract_flat_features(svm_train_loader)\n","svm_val_feats, svm_val_labels = extract_flat_features(svm_val_loader)\n","svm_test_feats, svm_test_labels = extract_flat_features(svm_test_loader)\n","\n","scaler = StandardScaler()\n","pca_components = 256\n","svm_train_feats = scaler.fit_transform(svm_train_feats)\n","svm_val_feats = scaler.transform(svm_val_feats)\n","svm_test_feats = scaler.transform(svm_test_feats)\n","\n","pca = PCA(n_components=pca_components, random_state=42)\n","svm_train_feats = pca.fit_transform(svm_train_feats)\n","svm_val_feats = pca.transform(svm_val_feats)\n","svm_test_feats = pca.transform(svm_test_feats)\n","\n","svm_clf = SGDClassifier(\n","    loss=\"hinge\",\n","    alpha=1e-5,\n","    max_iter=500,\n","    tol=1e-4,\n","    n_jobs=-1,\n","    class_weight=\"balanced\",\n","    random_state=42,\n","    verbose=0,\n",")\n","t0 = time.time()\n","svm_clf.fit(svm_train_feats, svm_train_labels)\n","svm_val_pred = svm_clf.predict(svm_val_feats)\n","svm_test_pred = svm_clf.predict(svm_test_feats)\n","\n","svm_val_acc = (svm_val_pred == svm_val_labels).mean()\n","svm_test_acc = (svm_test_pred == svm_test_labels).mean()\n","\n","print(f\"[SVM] Val acc={svm_val_acc:.4f} | Test acc={svm_test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"artEgIyD_CPP","executionInfo":{"status":"ok","timestamp":1765767769584,"user_tz":360,"elapsed":67734,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"30cc1c49-8c54-46f7-a71a-e4ead67f39f1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_test] samples=251, big=4, fine=16\n","[SVM] Val acc=0.3734 | Test acc=0.1195\n"]}]},{"cell_type":"markdown","source":["# 2. CNN"],"metadata":{"id":"_dEdP9jytviS"}},{"cell_type":"code","source":["num_fine_classes = len(full_train_dataset.fine_class_to_idx)\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        return self.classifier(x)\n","\n","\n","def train_cnn_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss, correct, total = 0.0, 0, 0\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","    return total_loss / total, correct / total\n","\n","\n","@torch.no_grad()\n","def eval_cnn_one_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss, correct, total = 0.0, 0, 0\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","    return total_loss / total, correct / total\n","\n","\n","cnn_model = SimpleCNN(num_fine_classes).to(device)\n","cnn_criterion = nn.CrossEntropyLoss()\n","cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n","cnn_epochs = 10\n","cnn_best_acc = 0.0\n","cnn_best_state = None\n","\n","for epoch in range(cnn_epochs):\n","    tr_loss, tr_acc = train_cnn_one_epoch(cnn_model, train_loader, cnn_optimizer, cnn_criterion, device)\n","    va_loss, va_acc = eval_cnn_one_epoch(cnn_model, val_loader, cnn_criterion, device)\n","    print(f\"[CNN] Epoch {epoch+1}/{cnn_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > cnn_best_acc:\n","        cnn_best_acc = va_acc\n","        cnn_best_state = {k: v.cpu() for k, v in cnn_model.state_dict().items()}\n","\n","if cnn_best_acc > 0:\n","    cnn_model.load_state_dict(cnn_best_state)\n","    cnn_model = cnn_model.to(device)\n","print(\"[CNN] Best val acc:\", cnn_best_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw909vbT9_vu","executionInfo":{"status":"ok","timestamp":1765768210100,"user_tz":360,"elapsed":440520,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"0675b9ae-9a22-4ac1-944c-8003defb031f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[CNN] Epoch 1/10 | Train loss=1.9027, acc=0.3878 | Val loss=1.7314, acc=0.4588\n","[CNN] Epoch 2/10 | Train loss=1.7874, acc=0.4165 | Val loss=1.7758, acc=0.4431\n","[CNN] Epoch 3/10 | Train loss=1.7290, acc=0.4362 | Val loss=1.6155, acc=0.4838\n","[CNN] Epoch 4/10 | Train loss=1.6778, acc=0.4553 | Val loss=1.5842, acc=0.5083\n","[CNN] Epoch 5/10 | Train loss=1.6388, acc=0.4670 | Val loss=1.5787, acc=0.4774\n","[CNN] Epoch 6/10 | Train loss=1.5964, acc=0.4783 | Val loss=1.5274, acc=0.5231\n","[CNN] Epoch 7/10 | Train loss=1.5637, acc=0.4879 | Val loss=1.5049, acc=0.5255\n","[CNN] Epoch 8/10 | Train loss=1.5265, acc=0.5001 | Val loss=1.4991, acc=0.5250\n","[CNN] Epoch 9/10 | Train loss=1.4992, acc=0.5150 | Val loss=1.4010, acc=0.5667\n","[CNN] Epoch 10/10 | Train loss=1.4820, acc=0.5175 | Val loss=1.5557, acc=0.5201\n","[CNN] Best val acc: 0.5667320902845927\n"]}]},{"cell_type":"markdown","source":["# 3. Fine tuned Resnet 18"],"metadata":{"id":"0WgRForQuO1w"}},{"cell_type":"markdown","source":["Stage 1: Freeze the backbone and only train the last layer"],"metadata":{"id":"Jnt2UfYm2xaw"}},{"cell_type":"code","source":["weights = models.ResNet18_Weights.IMAGENET1K_V1\n","model = models.resnet18(weights=weights)\n","\n","for p in model.parameters():\n","    p.requires_grad = False\n","\n","num_features = model.fc.in_features\n","num_fine_classes = len(full_train_dataset.fine_class_to_idx)\n","model.fc = nn.Linear(num_features, num_fine_classes)\n","\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)"],"metadata":{"id":"nSLSqBMTDiJc","executionInfo":{"status":"ok","timestamp":1765768210546,"user_tz":360,"elapsed":449,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b63603a-b5b9-47e0-8d33-7184b7f6f112"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 208MB/s]\n"]}]},{"cell_type":"code","source":["def train_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss, correct, total = 0.0, 0, 0\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","\n","    return total_loss / total, correct / total\n","\n","@torch.no_grad()\n","def eval_one_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss, correct, total = 0.0, 0, 0\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","\n","    return total_loss / total, correct / total"],"metadata":{"id":"fFmNFQRiDtTX","executionInfo":{"status":"ok","timestamp":1765768210592,"user_tz":360,"elapsed":36,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","best_val_acc = 0.0\n","best_state = None\n","\n","for epoch in range(num_epochs):\n","    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n","    va_loss, va_acc = eval_one_epoch(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n","          f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n","          f\"Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","\n","    if va_acc > best_val_acc:\n","        best_val_acc = va_acc\n","        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","\n","if best_state is not None:\n","    model.load_state_dict(best_state)\n","    model = model.to(device)\n","\n","print(\"Best val acc:\", best_val_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTz81IARD0ee","executionInfo":{"status":"ok","timestamp":1765768644814,"user_tz":360,"elapsed":434214,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"510b8856-588e-4d60-f186-309063df61b4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 | Train loss=0.8082, acc=0.7639 | Val loss=0.3644, acc=0.8827\n","Epoch 2/10 | Train loss=0.5206, acc=0.8329 | Val loss=0.3439, acc=0.8871\n","Epoch 3/10 | Train loss=0.4992, acc=0.8383 | Val loss=0.3098, acc=0.9009\n","Epoch 4/10 | Train loss=0.4647, acc=0.8493 | Val loss=0.2859, acc=0.9038\n","Epoch 5/10 | Train loss=0.4530, acc=0.8522 | Val loss=0.3001, acc=0.9038\n","Epoch 6/10 | Train loss=0.4570, acc=0.8495 | Val loss=0.3020, acc=0.9082\n","Epoch 7/10 | Train loss=0.4420, acc=0.8579 | Val loss=0.2914, acc=0.9097\n","Epoch 8/10 | Train loss=0.4348, acc=0.8579 | Val loss=0.2927, acc=0.9117\n","Epoch 9/10 | Train loss=0.4315, acc=0.8588 | Val loss=0.2706, acc=0.9117\n","Epoch 10/10 | Train loss=0.4259, acc=0.8625 | Val loss=0.2791, acc=0.9132\n","Best val acc: 0.9131501472031404\n"]}]},{"cell_type":"markdown","source":["Stage 2: Unfreeze layer 4 and fc"],"metadata":{"id":"eoyQ2Bsn3MyT"}},{"cell_type":"code","source":["for name, p in model.named_parameters():\n","    p.requires_grad = name.startswith(\"layer4\") or name.startswith(\"fc\")\n","\n","optimizer = optim.AdamW([\n","    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},\n","    {\"params\": model.fc.parameters(),     \"lr\": 5e-4},\n","], weight_decay=1e-4)\n","\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n","\n","num_epochs_ft = 5\n","best_val_acc_ft = 0.0\n","best_state_ft = None\n","\n","for epoch in range(num_epochs_ft):\n","    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n","    va_loss, va_acc = eval_one_epoch(model, val_loader, criterion, device)\n","\n","    print(f\"[FT] Epoch {epoch+1}/{num_epochs_ft} | \"\n","          f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n","          f\"Val loss={va_loss:.4f}, acc={va_acc:.4f}\", flush=True)\n","\n","    scheduler.step()\n","\n","    if va_acc > best_val_acc_ft:\n","        best_val_acc_ft = va_acc\n","        best_state_ft = {k: v.cpu() for k, v in model.state_dict().items()}\n","\n","if best_state_ft is not None:\n","    model.load_state_dict(best_state_ft)\n","    model = model.to(device)\n","\n","print(\"Best FT val acc:\", best_val_acc_ft)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsEyNa0c6sEJ","executionInfo":{"status":"ok","timestamp":1765768862648,"user_tz":360,"elapsed":217829,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"085777d8-b385-4ff6-c113-85eb35247db2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[FT] Epoch 1/5 | Train loss=0.4386, acc=0.8623 | Val loss=0.2385, acc=0.9279\n","[FT] Epoch 2/5 | Train loss=0.3091, acc=0.9001 | Val loss=0.2104, acc=0.9362\n","[FT] Epoch 3/5 | Train loss=0.2386, acc=0.9225 | Val loss=0.1791, acc=0.9446\n","[FT] Epoch 4/5 | Train loss=0.1933, acc=0.9364 | Val loss=0.1755, acc=0.9490\n","[FT] Epoch 5/5 | Train loss=0.1653, acc=0.9467 | Val loss=0.1630, acc=0.9534\n","Best FT val acc: 0.9533856722276742\n"]}]},{"cell_type":"markdown","source":["# 4. Fine tuned Mobile Net V3"],"metadata":{"id":"CjMnTZEiuvj-"}},{"cell_type":"code","source":["mb_weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V1\n","mobile_model = models.mobilenet_v3_large(weights=mb_weights)\n","mobile_model.classifier[-1] = nn.Linear(mobile_model.classifier[-1].in_features, num_fine_classes)\n","\n","for p in mobile_model.features.parameters():\n","    p.requires_grad = False\n","\n","mobile_model = mobile_model.to(device)\n","mobile_criterion = nn.CrossEntropyLoss()\n","mobile_optimizer = optim.Adam(mobile_model.classifier.parameters(), lr=1e-3)\n","mobile_epochs = 10\n","mobile_best_acc = 0.0\n","mobile_best_state = None\n","\n","for epoch in range(mobile_epochs):\n","    tr_loss, tr_acc = train_one_epoch(mobile_model, train_loader, mobile_optimizer, mobile_criterion, device)\n","    va_loss, va_acc = eval_one_epoch(mobile_model, val_loader, mobile_criterion, device)\n","    print(f\"[MobileNet] Epoch {epoch+1}/{mobile_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > mobile_best_acc:\n","        mobile_best_acc = va_acc\n","        mobile_best_state = {k: v.cpu() for k, v in mobile_model.state_dict().items()}\n","\n","if mobile_best_state is not None:\n","    mobile_model.load_state_dict(mobile_best_state)\n","    mobile_model = mobile_model.to(device)\n","print(\"[MobileNet] Best val acc:\", mobile_best_acc)\n","\n","# fine-tune: unfreeze backbone with smaller LR for a few epochs\n","for p in mobile_model.features.parameters():\n","    p.requires_grad = True\n","\n","mobile_ft_optimizer = optim.AdamW([\n","    {\"params\": mobile_model.features.parameters(), \"lr\": 3e-5, \"weight_decay\": 1e-4},\n","    {\"params\": mobile_model.classifier.parameters(), \"lr\": 3e-4, \"weight_decay\": 1e-4},\n","])\n","mobile_ft_epochs = 5\n","mobile_ft_best_acc = 0.0\n","mobile_ft_best_state = None\n","\n","for epoch in range(mobile_ft_epochs):\n","    tr_loss, tr_acc = train_one_epoch(mobile_model, train_loader, mobile_ft_optimizer, mobile_criterion, device)\n","    va_loss, va_acc = eval_one_epoch(mobile_model, val_loader, mobile_criterion, device)\n","    print(f\"[MobileNet-FT] Epoch {epoch+1}/{mobile_ft_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > mobile_ft_best_acc:\n","        mobile_ft_best_acc = va_acc\n","        mobile_ft_best_state = {k: v.cpu() for k, v in mobile_model.state_dict().items()}\n","\n","if mobile_ft_best_state is not None:\n","    mobile_model.load_state_dict(mobile_ft_best_state)\n","    mobile_model = mobile_model.to(device)\n","print(\"[MobileNet-FT] Best val acc:\", mobile_ft_best_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lxc-CSYLD2Df","executionInfo":{"status":"ok","timestamp":1765769519980,"user_tz":360,"elapsed":657329,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"c3ca0fe5-d11d-48a4-8525-1cbf3c315f19"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21.1M/21.1M [00:00<00:00, 128MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[MobileNet] Epoch 1/10 | Train loss=0.6201, acc=0.8048 | Val loss=0.2949, acc=0.9028\n","[MobileNet] Epoch 2/10 | Train loss=0.4456, acc=0.8567 | Val loss=0.2560, acc=0.9205\n","[MobileNet] Epoch 3/10 | Train loss=0.3962, acc=0.8708 | Val loss=0.2277, acc=0.9333\n","[MobileNet] Epoch 4/10 | Train loss=0.3715, acc=0.8795 | Val loss=0.2163, acc=0.9328\n","[MobileNet] Epoch 5/10 | Train loss=0.3547, acc=0.8852 | Val loss=0.2291, acc=0.9308\n","[MobileNet] Epoch 6/10 | Train loss=0.3416, acc=0.8926 | Val loss=0.2349, acc=0.9318\n","[MobileNet] Epoch 7/10 | Train loss=0.3254, acc=0.8966 | Val loss=0.2429, acc=0.9328\n","[MobileNet] Epoch 8/10 | Train loss=0.3173, acc=0.8985 | Val loss=0.2179, acc=0.9396\n","[MobileNet] Epoch 9/10 | Train loss=0.2926, acc=0.9072 | Val loss=0.2193, acc=0.9372\n","[MobileNet] Epoch 10/10 | Train loss=0.2909, acc=0.9072 | Val loss=0.2481, acc=0.9313\n","[MobileNet] Best val acc: 0.9396467124631992\n","[MobileNet-FT] Epoch 1/5 | Train loss=0.2335, acc=0.9245 | Val loss=0.1720, acc=0.9500\n","[MobileNet-FT] Epoch 2/5 | Train loss=0.1787, acc=0.9433 | Val loss=0.1716, acc=0.9553\n","[MobileNet-FT] Epoch 3/5 | Train loss=0.1669, acc=0.9467 | Val loss=0.1678, acc=0.9490\n","[MobileNet-FT] Epoch 4/5 | Train loss=0.1545, acc=0.9488 | Val loss=0.1666, acc=0.9553\n","[MobileNet-FT] Epoch 5/5 | Train loss=0.1396, acc=0.9543 | Val loss=0.1402, acc=0.9588\n","[MobileNet-FT] Best val acc: 0.9587831207065751\n"]}]},{"cell_type":"markdown","source":["# 5. Strategies evaluation and comparison"],"metadata":{"id":"2Tu-rwvOu4MG"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","from matplotlib.colors import PowerNorm, LogNorm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","@torch.no_grad()\n","def collect_fine_preds(model, loader, device):\n","    model.eval()\n","    y_true, y_pred = [], []\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        outputs = model(images)\n","        pred = outputs.argmax(dim=1).cpu().numpy()\n","\n","        y_true.extend(fine_labels.numpy())\n","        y_pred.extend(pred)\n","\n","    return np.array(y_true), np.array(y_pred)\n","\n","\n","def _plot_confusion_matrix(cm, class_names, title, normalize=False,\n","                           cmap=\"Blues\", use_log_for_counts=True, vmax_percentile=99):\n","\n","    cm_plot = cm.astype(np.float64)\n","\n","    if normalize:\n","        row_sums = cm_plot.sum(axis=1, keepdims=True)\n","        cm_plot = np.divide(cm_plot, row_sums, out=np.zeros_like(cm_plot), where=row_sums != 0) * 100.0\n","\n","    n = len(class_names)\n","\n","    fig_w = min(18, max(10, 0.65 * n))\n","    fig_h = min(18, max(7,  0.60 * n))\n","    plt.figure(figsize=(fig_w, fig_h))\n","\n","    if normalize:\n","\n","        vmax = 100.0\n","        norm = PowerNorm(gamma=0.5, vmin=0.0, vmax=vmax)\n","        im = plt.imshow(cm_plot, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","    else:\n","\n","        nonzero = cm_plot[cm_plot > 0]\n","        vmax = np.percentile(nonzero, vmax_percentile) if nonzero.size else 1.0\n","\n","        if use_log_for_counts:\n","\n","            masked = np.ma.masked_where(cm_plot == 0, cm_plot)\n","            norm = LogNorm(vmin=1, vmax=max(1, vmax))\n","            im = plt.imshow(masked, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","        else:\n","\n","            norm = PowerNorm(gamma=0.5, vmin=0.0, vmax=vmax)\n","            im = plt.imshow(cm_plot, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","\n","    plt.title(title)\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","\n","    tick_marks = np.arange(n)\n","    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\", fontsize=9)\n","    plt.yticks(tick_marks, class_names, fontsize=9)\n","\n","\n","    display_max = np.nanmax(cm_plot) if normalize else (np.nanmax(cm_plot[cm_plot > 0]) if np.any(cm_plot > 0) else 1.0)\n","    thresh = display_max * 0.5\n","\n","    for i in range(n):\n","        for j in range(n):\n","            val = cm_plot[i, j]\n","            if normalize:\n","                text = f\"{val:.1f}\"\n","                show = (val > 0)\n","            else:\n","                text = str(int(cm[i, j]))\n","                show = (cm[i, j] > 0)\n","\n","            if show:\n","                plt.text(j, i, text,\n","                         ha=\"center\", va=\"center\",\n","                         fontsize=8,\n","                         color=\"white\" if val > thresh else \"black\")\n","\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def evaluate_report(y_fine_true, y_fine_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"\"):\n","\n","    fine_names = [fine_idx_to_name[i] for i in range(len(fine_idx_to_name))]\n","\n","    print(f\"\\n=== {set_name} Fine-class Report  ===\")\n","    print(classification_report(y_fine_true, y_fine_pred, target_names=fine_names, zero_division=0))\n","\n","    cm_fine = confusion_matrix(y_fine_true, y_fine_pred, labels=np.arange(len(fine_names)))\n","    _plot_confusion_matrix(cm_fine, fine_names, title=f\"{set_name} Fine Confusion Matrix (Counts)\", normalize=False)\n","    _plot_confusion_matrix(cm_fine, fine_names, title=f\"{set_name} Fine Confusion Matrix (Row %)\", normalize=True)\n","\n","    y_big_true = np.array([fine_to_big[int(i)] for i in y_fine_true])\n","    y_big_pred = np.array([fine_to_big[int(i)] for i in y_fine_pred])\n","\n","    big_names = [big_idx_to_name[i] for i in range(len(big_idx_to_name))]\n","\n","    print(f\"\\n=== {set_name} Big-class Report  ===\")\n","    print(classification_report(y_big_true, y_big_pred, target_names=big_names, zero_division=0))\n","\n","    cm_big = confusion_matrix(y_big_true, y_big_pred, labels=np.arange(len(big_names)))\n","    _plot_confusion_matrix(cm_big, big_names, title=f\"{set_name} Big Confusion Matrix (Counts)\", normalize=False)\n","    _plot_confusion_matrix(cm_big, big_names, title=f\"{set_name} Big Confusion Matrix (Row %)\", normalize=True)\n","\n","print(\"\\n######## SVM VAL SET EVAL ########\")\n","evaluate_report(svm_val_labels, svm_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"SVM VAL\")\n","\n","print(\"\\n######## SVM TEST SET EVAL ########\")\n","evaluate_report(svm_test_labels, svm_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"SVM TEST\")\n","\n","print(\"\\n######## CNN VAL SET EVAL ########\")\n","y_cnn_val_true, y_cnn_val_pred = collect_fine_preds(cnn_model, val_loader, device)\n","evaluate_report(y_cnn_val_true, y_cnn_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"CNN VAL\")\n","\n","print(\"\\n######## CNN TEST SET EVAL ########\")\n","y_cnn_test_true, y_cnn_test_pred = collect_fine_preds(cnn_model, test_loader, device)\n","evaluate_report(y_cnn_test_true, y_cnn_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"CNN TEST\")\n","\n","y_val_true, y_val_pred = collect_fine_preds(model, val_loader, device)\n","print(\"\\n######## ResNet VAL SET EVAL ########\")\n","evaluate_report(y_val_true, y_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"VAL\")\n","\n","y_test_true, y_test_pred = collect_fine_preds(model, test_loader, device)\n","print(\"\\n######## ResNet TEST SET EVAL ########\")\n","evaluate_report(y_test_true, y_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"TEST\")\n","\n","print(\"\\n######## MobileNet VAL SET EVAL ########\")\n","y_mb_val_true, y_mb_val_pred = collect_fine_preds(mobile_model, val_loader, device)\n","evaluate_report(y_mb_val_true, y_mb_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"MobileNet VAL\")\n","\n","print(\"\\n######## MobileNet TEST SET EVAL ########\")\n","y_mb_test_true, y_mb_test_pred = collect_fine_preds(mobile_model, test_loader, device)\n","evaluate_report(y_mb_test_true, y_mb_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"MobileNet TEST\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dNaBrEG-wZEqn1Swl9fuss1YUvFZYvD5"},"id":"CIsjngHywpo2","executionInfo":{"status":"ok","timestamp":1765769595184,"user_tz":360,"elapsed":75200,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"98dac2b7-c8c7-47cb-d6a8-378d61d6167a"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n","# install can take a minute\n","\n","import os\n","# @title Convert Notebook to PDF. Save Notebook to given directory\n","NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/Final\" # @param {type:\"string\"}\n","NOTEBOOK_NAME = \"Imagenet.ipynb\" # @param {type:\"string\"}\n","#------------------------------------------------------------------------------#\n","from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)\n","NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n","assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n","!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n","!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n","NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n","assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n","print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"],"metadata":{"id":"AavJeRSwv32f","executionInfo":{"status":"ok","timestamp":1765769595221,"user_tz":360,"elapsed":2,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import time\n","import cv2\n","import torch.nn.functional as F\n","import io\n","import base64\n","from google.colab.output import eval_js\n","from IPython.display import display, Javascript"],"metadata":{"id":"J7RK3NiJEBdE","executionInfo":{"status":"ok","timestamp":1765769788606,"user_tz":360,"elapsed":69,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["\"\"\"Webcam inference (optional, run manually in Colab/local)\"\"\"\n","\n","\n","def predict_single_pil(pil_img, model, transform, device):\n","    model.eval()\n","    with torch.no_grad():\n","        x = transform(pil_img).unsqueeze(0).to(device)\n","        logits = model(x)\n","        probs = F.softmax(logits, dim=1)[0]\n","        fine_idx = torch.argmax(probs).item()\n","        big_idx = fine_to_big.get(fine_idx, -1)\n","        return fine_idx, big_idx, probs[fine_idx].item()\n","\n","\n","def run_webcam(model, device, window_name=\"Garbage classifier\"):\n","    cap = cv2.VideoCapture(0)\n","    if not cap.isOpened():\n","        print(\"Cannot open webcam\")\n","        return\n","    try:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            pil_img = Image.fromarray(rgb)\n","            fine_idx, big_idx, prob = predict_single_pil(pil_img, model, val_test_transform, device)\n","            fine_name = fine_idx_to_name.get(fine_idx, \"unknown\")\n","            big_name = big_idx_to_name.get(big_idx, \"unknown\")\n","            text = f\"{big_name} / {fine_name} ({prob*100:.1f}%)\"\n","            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n","                        0.7, (0, 255, 0), 2, cv2.LINE_AA)\n","            cv2.imshow(window_name, frame)\n","            if cv2.waitKey(1) & 0xFF == ord('q'):\n","                break\n","    finally:\n","        cap.release()\n","        cv2.destroyAllWindows()"],"metadata":{"id":"vqLz-O3NEHnq","executionInfo":{"status":"ok","timestamp":1765769805775,"user_tz":360,"elapsed":17,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\"\"\"Colab camera capture (JS widget) for inference\"\"\"\n","\n","def _colab_take_photo():\n","    js_code = r\"\"\"\n","    async function takePhoto() {\n","      const div = document.createElement('div');\n","      const video = document.createElement('video');\n","      const btn = document.createElement('button');\n","      btn.textContent = '拍照';\n","      div.appendChild(video);\n","      div.appendChild(btn);\n","      document.body.appendChild(div);\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","      video.srcObject = stream;\n","      await video.play();\n","      await new Promise((resolve) => btn.onclick = resolve);\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getTracks().forEach(t => t.stop());\n","      const data = canvas.toDataURL('image/png');\n","      div.remove();\n","      return data;\n","    }\n","    takePhoto();\n","    \"\"\"\n","    data = eval_js(js_code)\n","    header, encoded = data.split(\",\", 1)\n","    return Image.open(io.BytesIO(base64.b64decode(encoded))).convert(\"RGB\")\n","\n","\n","def colab_capture_and_predict(model):\n","    pil_img = _colab_take_photo()\n","    fine_idx, big_idx, prob = predict_single_pil(pil_img, model, val_test_transform, device)\n","    fine_name = fine_idx_to_name.get(fine_idx, \"unknown\")\n","    big_name = big_idx_to_name.get(big_idx, \"unknown\")\n","    print(f\"big group: {big_name} | small group: {fine_name} | Confidence: {prob*100:.1f}%\")\n","\n","colab_capture_and_predict(mobile_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"0hcRElXtFKum","executionInfo":{"status":"ok","timestamp":1765772170534,"user_tz":360,"elapsed":3847,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"10f0ff68-d7ca-43d2-9461-b662f758cee7"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["big group: recycling | small group: recycling/plastic | Confidence: 80.3%\n"]}]},{"cell_type":"code","source":["try:\n","    import gradio as gr\n","except ImportError:\n","    gr = None\n","\n","\n","def gradio_predict(image, model):\n","    if image is None:\n","        return \"No image received. Please capture or upload again.\"\n","    # image: numpy array (H, W, C) in RGB\n","    pil_img = Image.fromarray(image.astype(\"uint8\"))\n","    fine_idx, big_idx, prob = predict_single_pil(pil_img, model, val_test_transform, device)\n","    fine_name = fine_idx_to_name.get(fine_idx, \"unknown\")\n","    big_name = big_idx_to_name.get(big_idx, \"unknown\")\n","    return f\"Big: {big_name} | Fine: {fine_name} | Confidence: {prob*100:.1f}%\"\n","\n","\n","def launch_gradio(model):\n","    if gr is None:\n","        raise ImportError(\"请先安装 gradio: pip install gradio\")\n","    iface = gr.Interface(\n","        fn=lambda img: gradio_predict(img, model),\n","        inputs=gr.Image(\n","            sources=[\"webcam\", \"upload\"],  # single capture or upload\n","            type=\"numpy\",\n","            streaming=False,       # capture one frame at a time\n","            mirror_webcam=False,   # disable mirroring for rear camera\n","            label=\"Webcam / Upload\"\n","        ),\n","        outputs=\"text\",\n","        title=\"Garbage Classifier\",\n","        description=\"Click capture or upload an image to classify big/fine category; repeat as needed\",\n","        allow_flagging=\"never\"\n","    )\n","    iface.launch(share=True)  # share=True\n","launch_gradio(mobile_model)  # 或传入 model/cnn_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":633},"id":"AgKjVYgtIC2d","executionInfo":{"status":"ok","timestamp":1765772395954,"user_tz":360,"elapsed":2581,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"ad408896-dd4f-4f5f-e059-3aeb83ecd129"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://66595c7648217620e8.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://66595c7648217620e8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":[],"metadata":{"id":"GfWjX6H0RiWw"}},{"cell_type":"code","source":[],"metadata":{"id":"wJVg_hoaRh9Z"},"execution_count":null,"outputs":[]}]}